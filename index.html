<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Yang's Homepage</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">

	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url(images/about2.jpg);"></div>
				<h1 id="colorlib-logo"><a href="index.html">Yang Zhou</a></h1>
				<span class="position"><a href="https://scholar.google.com/citations?user=UuwugFEAAAAJ&hl=en">[Google Scholar]</a> <a href="https://github.com/yzhou359">[Github]</a></span>
				<li class="position"><a href="#">Research Scientist</a></li>
				<li class="location">Adobe Research</li>
				<li class="location" style="margin-bottom: 2em">San Jose, CA</li>
				<li class="location" style="margin-bottom: 4em">Email: <a href="mailto:yangzhou@cs.umass.edu">yazhou@adobe.com</a></li>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<li class="active"><a href="#" data-nav-section="home">Home</a></li>
						<li><a href="#" data-nav-section="publication">Publication</a></li>
						<li><a href="#" data-nav-section="work">Experience</a></li>
						<li><a href="#" data-nav-section="contact">Contact</a></li>
					</ul>
				</div>
			</nav>

			<div class="colorlib-footer" style="margin-top: 250px; font-size: 12px" >
				<p><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=Vm1c13efnAY97Yw38hh9cXYljhmSL85rjox-8fYwNBI&cl=ffffff&w=a"></script></p>
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span></small></p>
				<ul>
					<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin2"></i></a></li>
				</ul>
			</div>

		</aside>

		<div id="colorlib-main">
			<section id="colorlib-hero" class="js-fullheight" data-section="home">
				<div class="flexslider js-fullheight">
					<ul class="slides">
				   	<!--li style="background-image: url(<./images/img_bg_2.jpg);"-->
				   	<li style="background-image: url();">
				   		<div class="overlay"></div>
				   		<div class="container-fluid">
				   			<div class="row">
					   			<div class="col-md-offset-3 col-md-pull-3 col-sm-12 col-xs-12 js-fullheight slider-text">
					   				<div class="slider-text-inner js-fullheight">
					   					<div class="desc">
						   					<h1>Hi! <br>I'm Yang Zhou</h1>
						   					<h2>I am a research scientist at <a href="https://research.adobe.com/">Adobe Research</a>, specializing in video generation, visual effects, and developing applications for digital avatars. </h2>
						   					<h2>I completed my Ph.D. study in the Computer Graphics Research Group at UMass Amherst, advised by Prof. <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>. I obtained my master's degree from Georgia Institute of Technology and my master & bachelor's degree from Shanghai Jiao Tong University, advised by Prof. <a href="https://weiyaolin.github.io/">Weiyao Lin</a>.</h2>

						   					<h2>I work in the areas of computer vision, computer graphics and deep learning. In particular, I am interested in using deep learning techniques to help artists, stylists and animators to make better design. In particular, I am interested in the field of <a>video generation</a>, <a>digital human and characters</a>, <a>audio-visual learning</a>, and <a>3D generation</a>.  </h2>

						   					<!-- <h2><b> <span style="color:red">[NEW!]</span> I'm looking for active interns to work on Digital Human related research projects in summer 2021, especially on human pose/hands tracking, animation etc. Please feel free to reach out to learn more.</b></h2> -->

												<p><a href="./cv/CV_YANG_ZHOU_202410.pdf" class="btn btn-primary btn-learn">Download CV <i class="icon-download4"></i></a></p>
											</div>
					   				</div>
					   			</div>
					   		</div>
				   		</div>
				   	</li>
				  	</ul>
			  	</div>
			</section>

			<section class="colorlib-contact" data-section="news" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<span class="heading-meta">Recent</span>
							<h2 class="colorlib-heading">News</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
							<div class="blog-entry" style="width: 80%;">
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Oct. 2024] 2 papers accepted by NeurIPS 2024.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [July 2024] 2 papers accepted by ECCV 2024.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Jan. 2024] 1 papers accepted by 3DV 2024.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Nov. 2023] Our video super-resolution project <a href="">#ProjResUp</a> was presented at Adobe Max 2023 (Sneak Peak) <a href="https://www.youtube.com/watch?v=MnOVEcst_U4&themeRefresh=1">[Youtube Link]</a> <a href="https://www.theverge.com/2023/10/11/23912480/adobe-max-sneak-project-resup-ai-video-upscaling-preview">[Press]</a></p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Nov. 2023] Our paper 3D generation foundation model "LRM: Large Reconstruction Model for Single Image to 3D" has been accepted by ICLR 2023.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Oct. 2023] 2 papers accepted by ICCV 2023.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Oct. 2022] 1 paper accepted by SIGGRAPH ASIA 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Oct. 2022] 2 papers accepted by ECCV 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Jun. 2022] 2 papers accepted by CVPR 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [May 2021] Start my new journey at Adobe Research as a full-time research scientist.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Mar. 2021] Gave a talk on deep learning architectures for character animation at Intelligent Graphics Lab, Chinese Academy of Sciences.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Nov. 2020] Our summer intern project <a href="https://twitter.com/hashtag/OnTheBeatSneak?src=hashtag_click">#OnTheBeatSneak</a> was presented at Adobe MAX 2020 (Sneak Peek). <a href="https://www.youtube.com/watch?v=R-0w3IuGEKU&feature=emb_title">[Quick Look]</a> <a href="https://www.youtube.com/watch?v=NMHLAVjyFxo">[Full Youtube Link]</a> <a href="https://www.protocol.com/adobe-max-ai-video-editing">[Press]</a></p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Aug. 2020] Our paper <a href="#MakeItTalk">MakeItTalk</a> accepted by SIGGRAPH ASIA 2020. <a href="https://www.youtube.com/watch?v=vUMGKASgbf8">[Video]</a></p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Apr. 2020] Our paper <a href="#RigNet">RigNet</a> accepted by SIGGRAPH 2020. <a href="https://www.youtube.com/watch?v=J90VETgWIDg&feature=emb_logo">[Video]</a></p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Nov. 2019] Our summer intern project <a href="https://twitter.com/hashtag/SweetTalkSneak?src=hashtag_click">#SweetTalkSneak</a> was presented at Adobe MAX 2019 (Sneak Peek). <a href="https://www.youtube.com/watch?v=JKZcYqZA8oo">[Youtube Link]</a> <a href="https://techcrunch.com/2019/11/05/adobes-project-sweet-talk-makes-portraits-come-alive/">[Press]</a></p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Aug. 2019] Our paper on <a href="#AnimationSkeleton">Animation Skeleton Prediction</a> accepted by 3DV 2019.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jul. 2019] Our paper <a href="#SceneGraphNet">SceneGraphNet</a> accepted by ICCV 2019.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jun. 2019] Joined Adobe CIL (Seattle) as a summer intern.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jun. 2018] Joined Wayfair Next Research as a summer intern and fall co-op intern.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Apr. 2018] Our paper <a href="#VisemeNet">VisemeNet</a> accepted by SIGGRAPH 2018. <a href="https://www.youtube.com/watch?v=kk2EnyMD3mo">[Video]</a></p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-experience" data-section="publication">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Academia</span>
							<h2 class="colorlib-heading">Publications</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
				         <div class="timeline-centered">.

							<h2>Video Generation and Visual Effects 
								<!-- <span onclick="toggleSection()" style="font-size: 12px;">Show/Hide Content</span> -->
							</h2>
							
							<table id="toggleSection" class="imgtable"><tbody><tr><td>
								<img src="images/desai_pavdm.gif" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://progressive-autoregressive-vdm.github.io/">Progressive Autoregressive Video Diffusion Models</a></b> 
									  <br>
									  Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, Yang Zhou
									  <br>
										<i>Arxiv 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2410.08151">PDF</a>&nbsp;&nbsp;
										<a href="https://progressive-autoregressive-vdm.github.io/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table id="toggleSection" class="imgtable"><tbody><tr><td>
								<img src="images/customize-a-video.gif" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://ryx19th.github.io/customize-a-video/">Customize-a-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models</a></b> 
									  <br>
									  Ren, Yixuan, <b>Yang Zhou</b>, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava
									  <br>
										<i>ECCV 2024</i>
									  <br>
										<a href="https://ryx19th.github.io/customize-a-video/static/pdf/paper.pdf">PDF</a>&nbsp;&nbsp;
										<a href="https://ryx19th.github.io/customize-a-video/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/mingi.gif" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://kwonminki.github.io/HARIVO/">HARIVO: Harnessing Text-to-Image Models for Video Generation</a></b> 
									  <br>
									  Mingi Kwon, Seoung Wug Oh, <b>Yang Zhou</b>, Joon-Young Lee, Difan Liu, Haoran Cai, Baqiao Liu, Feng Liu, Youngjung Uh
									  <br>
										<i>ECCV 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2410.07763">PDF</a>&nbsp;&nbsp;
										<a href="https://kwonminki.github.io/HARIVO/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/videogigagan.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://videogigagan.com/">VideoGigaGAN: Towards Detail-Rich Video Super-Resolution</a></b> 
									  <br>
									  Xu, Yiran, Taesung Park, Richard Zhang, Yang Zhou, Eli Shechtman, Feng Liu, Jia-Bin Huang, and Difan Liu
									  <br>
										<i>Arxiv 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2404.12388">PDF</a>&nbsp;&nbsp;
										<a href="https://videogigagan.com/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/jumpcut.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://jeanne-wang.github.io/jumpcutsmoothing/">Jump Cut Smoothing for Talking Heads</a></b> 
									  <br>
									  Wang, Xiaojuan, Taesung Park, Yang Zhou, Eli Shechtman, and Richard Zhang
									  <br>
										<i>Arxiv 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2401.04718">PDF</a>&nbsp;&nbsp;
										<a href="https://jeanne-wang.github.io/jumpcutsmoothing/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/actanywhere.gif" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://actanywhere.github.io/">ActAnywhere: Subject-Aware Video Background Generation</a></b> 
									  <br>
									  Pan, Boxiao, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, <b>Yang Zhou</b>, Leonidas J. Guibas, and Jimei Yang
									  <br>
										<i>NeurIPS 2024</i>
									  <br>
										<a href="">PDF</a>&nbsp;&nbsp;
										<a href="https://actanywhere.github.io/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/video_reenact.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://yzhou359.github.io/video_reenact/">Audio-driven Neural Gesture Reenactment with Video Motion Graphs</a></b> 
									  <br>
									  Yang Zhou, Jimei Yang, Dingzeyu Li, Jun Saito, Deepali Aneja, and Evangelos Kalogerakis
									  <br>
										<i>CVPR 2022</i>
									  <br>
										<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Audio-Driven_Neural_Gesture_Reenactment_With_Video_Motion_Graphs_CVPR_2022_paper.pdf">PDF</a>&nbsp;&nbsp;
										<a href="https://yzhou359.github.io/video_reenact/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<h2  style="margin-top: 50px;">Digital Human and Characters 
								<!-- <span onclick="toggleSection()" style="font-size: 12px;">Show/Hide Content</span> -->
							</h2>


							<table class="imgtable"><tbody><tr><td>
								<img src="images/humanlrm.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://zzweng.github.io/humanlrm/">Template-Free Single-View 3D Human Digitalization with Diffusion-Guided LRM</a></b> 
									  <br>
									  Weng, Zhenzhen, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, and Jimei Yang
									  <br>
										<i>Arxiv 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2401.12175">PDF</a>&nbsp;&nbsp;
										<a href="https://zzweng.github.io/humanlrm/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/grip.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://grip.is.tue.mpg.de/">GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency</a></b> 
									  <br>
									  Taheri, Omid, Yi Zhou, Dimitrios Tzionas, Yang Zhou, Duygu Ceylan, Soren Pirk, and Michael J. Black. 
									  <br>
										<i>3DV 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2308.11617">PDF</a>&nbsp;&nbsp;
										<a href="https://grip.is.tue.mpg.de/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/shaowei_contactgen.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://stevenlsw.github.io/contactgen/">ContactGen: Generative Contact Modeling for Grasp Generation</a></b> 
									  <br>
									  Liu, Shaowei, Yang Zhou, Jimei Yang, Saurabh Gupta, and Shenlong Wang
									  <br>
										<i>ICCV 2023</i>
									  <br>
										<a href="https://arxiv.org/abs/2310.03740">PDF</a>&nbsp;&nbsp;
										<a href="https://stevenlsw.github.io/contactgen/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/morig.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://zhan-xu.github.io/motion-rig/">MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</a></b> 
									  <br>
									  Zhan Xu, Yang Zhou, Li Yi, and Evangelos Kalogerakis
									  <br>
										<i>SIGGRAPH ASIA 2022</i>
									  <br>
										<a href="https://arxiv.org/abs/2210.09463">PDF</a>&nbsp;&nbsp;
										<a href="https://zhan-xu.github.io/motion-rig/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/skel-free_motion_trans.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://zycliao.com/sfpt/">Skeleton-free Pose Transfer for Stylized 3D Characters</a></b> 
									  <br>
									  Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard Pons-Moll, and Yang Zhou
									  <br>
										<i>ECCV 2022</i>
									  <br>
										<a href="https://zycliao.com/sfpt/sfpt.pdf">PDF</a>&nbsp;&nbsp;
										<a href="https://zycliao.com/sfpt/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/visdb.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://arxiv.org/pdf/2208.10652.pdf">Learning Visibility for Robust Dense Human Body Estimation</a></b> 
									  <br>
									  Chun-Han Yao, Jimei Yang, Duygu Ceylan, Yi Zhou, Yang Zhou, and Ming-Hsuan Yang
									  <br>
										<i>ECCV 2022</i>
									  <br>
										<a href="https://arxiv.org/pdf/2208.10652.pdf">PDF</a>&nbsp;&nbsp;
										<a href="https://github.com/chhankyao/visdb">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/APES.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://zhan-xu.github.io/parts/">APES: Articulated Part Extraction from Sprite Sheets</a></b> 
									  <br>
									  Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, Li Yi, and Evangelos Kalogerakis
									  <br>
										<i>CVPR 2022</i>
									  <br>
										<a href="https://arxiv.org/abs/2206.02015">PDF</a>&nbsp;&nbsp;
										<a href="https://zhan-xu.github.io/parts/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							
							
							<table class="imgtable"><tbody><tr><td>
								<img src="./images/makeittalk_new.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://people.umass.edu/~yangzhou/MakeItTalk/">MakeItTalk: Speaker-Aware Talking-Head Animation</a></b> 
									  <br>
									  Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li
									  <br>
										<i>SIGGRAPH ASIA 2020</i>
									  <br>
										<a href="https://arxiv.org/abs/2004.12992">PDF</a>&nbsp;&nbsp;
										<a href="https://people.umass.edu/~yangzhou/MakeItTalk/">Project</a>&nbsp;&nbsp
										<a href="https://youtu.be/OU6Ctzhpc6s">Video 1</a>&nbsp;&nbsp
										<a href="https://www.youtube.com/watch?v=vUMGKASgbf8">Video 2</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/rignet.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://zhan-xu.github.io/rig-net/">RigNet: Neural Rigging for Articulated Characters</a></b> 
									  <br>
									  Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh
									  <br>
										<i>SIGGRAPH 2020</i>
									  <br>
										<a href="images/rignet.pdf">PDF</a>&nbsp;&nbsp;
										<a href="https://zhan-xu.github.io/rig-net/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/3dv_rigging.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://people.cs.umass.edu/~zhanxu/projects/AnimSkelVolNet//">Predicting Animation Skeletons for 3D Articulated Models via Volumetric Nets</a></b> 
									  <br>
									  Zhan Xu, Yang Zhou, Evangelos Kalogerakis, and Karan Singh. 
									  <br>
										<i>3DB 2019</i>
									  <br>
										<a href="https://arxiv.org/abs/1908.08506">PDF</a>&nbsp;&nbsp;
										<a href="https://people.cs.umass.edu/~zhanxu/projects/AnimSkelVolNet//">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/visemenet.jpg" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://people.umass.edu/~yangzhou/visemenet/">VisemeNet: Audio-Driven Animator-Centric Speech Animation</a></b> 
									  <br>
									  Yang Zhou, Zhan Xu, Chris Landreth, Subhransu Maji, Evangelos Kalogerakis, and Karan Singh
									  <br>
										<i>SIGGRAPH 2018</i>
									  <br>
										<a href="https://arxiv.org/abs/1805.09488">PDF</a>&nbsp;&nbsp;
										<a href="https://people.umass.edu/~yangzhou/visemenet/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>






							<h2 style="margin-top: 50px;">3D Object and Scene Synthesis 
								<!-- <span onclick="toggleSection()" style="font-size: 12px;">Show/Hide Content</span> -->
							</h2>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/dmesh.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://sonsang.github.io/dmesh-project/">DMesh: A Differentiable Mesh Representation</a></b> 
									  <br>
									  Sanghyun Son, Matheus Gadelha, Yang Zhou, Zexiang Xu, Ming Lin, Yi Zhou
									  <br>
										<i>NeurIPS 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2404.13445">PDF</a>&nbsp;&nbsp;
										<a href="https://sonsang.github.io/dmesh-project/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/lrm.gif" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://yiconghong.me/LRM/">LRM: Large Reconstruction Model for Single Image to 3D</a></b> 
									  <br>
									  Hong, Yicong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan
									  <br>
										<i>ICLR 2024</i>
									  <br>
										<a href="https://arxiv.org/abs/2311.04400">PDF</a>&nbsp;&nbsp;
										<a href="https://yiconghong.me/LRM/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="images/yicong_embody.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>

								<td align="left"><b><style="font-size:100%">
									  <a  href="https://arxiv.org/abs/2307.12335">Learning Navigational Visual Representations with Semantic Map Supervision</a></b> 
									  <br>
									  Hong, Yicong, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, and Hao Tan
									  <br>
										<i>ICCV 2023</i>
									  <br>
										<a href="https://arxiv.org/abs/2307.12335">PDF</a>&nbsp;&nbsp;
								</td></tr></tbody>
							</table>

							<table class="imgtable"><tbody><tr><td>
								<img src="./images/scenegraphnet.png" alt="1" width="240px" >&nbsp;&nbsp;&nbsp;&nbsp;</td>
								<td align="left"><b><style="font-size:100%">
									  <a  href="https://people.umass.edu/~yangzhou/scenegraphnet/">SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation</a></b> 
									  <br>
									  Yang Zhou, Zachary While, and Evangelos Kalogerakis
									  <br>
										<i>ICCV 2019</i>
									  <br>
										<a href="https://arxiv.org/abs/1907.11308">PDF</a>&nbsp;&nbsp;
										<a href="https://people.umass.edu/~yangzhou/scenegraphnet/">Project</a>&nbsp;&nbsp
								</td></tr></tbody>
							</table>

							

					         <article class="timeline-entry begin" data-animate-effect="fadeInBottom">
					            <!--div class="timeline-entry-inner">
					               <div class="timeline-icon color-none">
					               </div>
					            </div -->
					         </article>
					      </div>
					   </div>
				   </div>
				</div>
			</section>


			<section class="colorlib-blog" data-section="work" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Work</span>
							<h2 class="colorlib-heading">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Media Intelligent Lab</a></h3>
									<span><small>May 2021 </small> | <small> Research Scientist </small> </span>
									<p>Working on various research projects including video generation, digital human, 3D generation, etc.</p>
									<!-- <p><b>Body Turner Behavior</b> shipped in Adobe Character Animator. <a href="https://helpx.adobe.com/adobe-character-animator/using/whats-new.html">[Link]</a>  -->
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Media Intelligent Lab</a></h3>
									<span><small>June, 2020 </small> | <small> Intern </small> </span>
									<p>Collaborate with researchers on 3D facial/skeleton animations based on deep learning approaches.</p>
									<p>Our intern project #OnTheBeatSneak was presented at <a href="https://blog.adobe.com/en/publish/2020/10/21/max-sneaks-2020-where-creativity-and-innovation-knows-no-bounds.html#gs.kjk07b">Adobe MAX 2020 (Sneak Peek)</a>.</p>
									<p><a href="https://www.youtube.com/watch?v=R-0w3IuGEKU&feature=emb_title">[Quick Look]</a> <a href="https://www.youtube.com/watch?v=NMHLAVjyFxo">[Full Youtube Link]</a> <a href="https://www.protocol.com/adobe-max-ai-video-editing">[Press]</a></p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Creative Intelligence Lab</a></h3>
									<span><small>June, 2019 </small> | <small> Intern </small> </span>
									<p>Collaborate with researchers on audio-driven cartoon and real human facial animations and lip-sync technologies based on deep learning approaches.</p>
									<p>Our intern project #SweetTalk was presented at <a href="https://theblog.adobe.com/adobe-max-sneaks-2019/?scid=4e31cfc1-b79d-4cb4-b0e3-cc4907d0e054&mv=social&mv2=owned_social">Adobe MAX 2019 (Sneak Peek)</a>.</p>
									<p><a href="https://www.youtube.com/watch?v=JKZcYqZA8oo">[Youtube Link]</a> <a href="https://techcrunch.com/2019/11/05/adobes-project-sweet-talk-makes-portraits-come-alive/">[Press]</a></p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/wayfair.jpg" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://tech.wayfair.com/category/wayfair-next/">Wayfair, Inc | Wayfair Next Research</a></h3>
									<span><small>June, 2018 </small> | <small> Research Intern </small> </span>
									<p>Working on 3D scene systhesis based on deep learning approaches.</p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/netease.jpg" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://game.163.com/en/">NetEase Game, Inc</a></h3>
									<span><small>June, 2015 </small> | <small> Management Trainee </small> </span>
									<p>Working on mobile game design, especially on profit models and user-experiences.</p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<!-- <section class="colorlib-work" data-section="work">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Work</span>
							<h2 class="colorlib-heading animate-box">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/img-1.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="work.html">Work 01</a></h3>
										<span>Website</span>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section> -->


			<section class="colorlib-contact" data-section="contact" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<span class="heading-meta">Best way to</span>
							<h2 class="colorlib-heading">Contact Me</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
							<div class="blog-entry" style="width: 80%;">
								<div class="desc" style="margin-left: 20px">
									<p>Best way to reach me is to send an <a href="mailto:yazhou@adobe.com/">Email</a></p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>


	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	<script>
		function toggleSection() {
			var content = document.getElementById("toggleSection");
			if (content.style.display === "none" || content.style.display === "") {
				content.style.display = "block";
			} else {
				content.style.display = "none";
			}
		}
	</script>

	</body>
</html>

