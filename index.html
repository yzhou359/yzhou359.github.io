<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Yang's Homepage</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">

	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url(images/about2.jpg);"></div>
				<h1 id="colorlib-logo"><a href="index.html">Yang Zhou</a></h1>
				<span class="position"><a href="https://scholar.google.com/citations?user=UuwugFEAAAAJ&hl=en">[Google Scholar]</a> <a href="https://github.com/yzhou359">[Github]</a></span>
				<li class="position"><a href="#">Research Scientist</a></li>
				<li class="location">Adobe Research</li>
				<li class="location" style="margin-bottom: 2em">San Jose, CA</li>
				<li class="location" style="margin-bottom: 4em">Email: <a href="mailto:yangzhou@cs.umass.edu">yazhou@adobe.com</a></li>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<li class="active"><a href="#" data-nav-section="home">Home</a></li>
						<li><a href="#" data-nav-section="publication">Publication</a></li>
						<li><a href="#" data-nav-section="work">Experience</a></li>
						<li><a href="#" data-nav-section="contact">Contact</a></li>
					</ul>
				</div>
			</nav>

			<div class="colorlib-footer" style="margin-top: 250px; font-size: 12px" >
				<p><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=Vm1c13efnAY97Yw38hh9cXYljhmSL85rjox-8fYwNBI&cl=ffffff&w=a"></script></p>
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span></small></p>
				<ul>
					<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin2"></i></a></li>
				</ul>
			</div>

		</aside>

		<div id="colorlib-main">
			<section id="colorlib-hero" class="js-fullheight" data-section="home">
				<div class="flexslider js-fullheight">
					<ul class="slides">
				   	<!--li style="background-image: url(<./images/img_bg_2.jpg);"-->
				   	<li style="background-image: url();">
				   		<div class="overlay"></div>
				   		<div class="container-fluid">
				   			<div class="row">
					   			<div class="col-md-offset-3 col-md-pull-3 col-sm-12 col-xs-12 js-fullheight slider-text">
					   				<div class="slider-text-inner js-fullheight">
					   					<div class="desc">
						   					<h1>Hi! <br>I'm Yang Zhou</h1>
						   					<h2>I am a research scientist in the Dynamic Media Organization (DMO) at <a href="https://research.adobe.com/">Adobe Research</a>. </h2>
						   					<h2>I completed my Ph.D. study in the Computer Graphics Research Group at UMass Amherst, advised by Prof. <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a>. I obtained my master's degree from Georgia Institute of Technology and my master & bachelor's degree from Shanghai Jiao Tong University, advised by Prof. <a href="https://weiyaolin.github.io/">Weiyao Lin</a>.</h2>

						   					<h2>I work in the areas of computer graphics, computer vision and deep learning. In particular, I am interested in using deep learning techniques to help artists, stylists and animators to make better design. In particular, I am interested in the field of <a>digital human</a>, <a>character animation</a>, <a>audio-visual learning</a>, <a>image/video translation</a> and <a>rigging/skinning</a>.  </h2>

						   					<!-- <h2><b> <span style="color:red">[NEW!]</span> I'm looking for active interns to work on Digital Human related research projects in summer 2021, especially on human pose/hands tracking, animation etc. Please feel free to reach out to learn more.</b></h2> -->

												<p><a href="./cv/CV_YANG_ZHOU.pdf" class="btn btn-primary btn-learn">Download CV <i class="icon-download4"></i></a></p>
											</div>
					   				</div>
					   			</div>
					   		</div>
				   		</div>
				   	</li>
				  	</ul>
			  	</div>
			</section>

			<section class="colorlib-contact" data-section="news" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<span class="heading-meta">Recent</span>
							<h2 class="colorlib-heading">News</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
							<div class="blog-entry" style="width: 80%;">
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Oct. 2022] 1 paper accepted by SIGGRAPH ASIA 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; <span>[NEW!]</span> [Oct. 2022] 2 papers accepted by ECCV 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Jun. 2022] 2 papers accepted by CVPR 2022.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [May 2021] Start my new journey at Adobe Research as a full-time research scientist.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Mar. 2021] Gave a talk on deep learning architectures for character animation at Intelligent Graphics Lab, Chinese Academy of Sciences.</p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Nov. 2020] Our summer intern project <a href="https://twitter.com/hashtag/OnTheBeatSneak?src=hashtag_click">#OnTheBeatSneak</a> was presented at Adobe MAX 2020 (Sneak Peek). <a href="https://www.youtube.com/watch?v=R-0w3IuGEKU&feature=emb_title">[Quick Look]</a> <a href="https://www.youtube.com/watch?v=NMHLAVjyFxo">[Full Youtube Link]</a> <a href="https://www.protocol.com/adobe-max-ai-video-editing">[Press]</a></p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Aug. 2020] Our paper <a href="#MakeItTalk">MakeItTalk</a> accepted by SIGGRAPH ASIA 2020. <a href="https://www.youtube.com/watch?v=vUMGKASgbf8">[Video]</a></p>
								</div>
								<div class="news" style="margin-left: 20px">
									<p>&#9658; [Apr. 2020] Our paper <a href="#RigNet">RigNet</a> accepted by SIGGRAPH 2020. <a href="https://www.youtube.com/watch?v=J90VETgWIDg&feature=emb_logo">[Video]</a></p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Nov. 2019] Our summer intern project <a href="https://twitter.com/hashtag/SweetTalkSneak?src=hashtag_click">#SweetTalkSneak</a> was presented at Adobe MAX 2019 (Sneak Peek). <a href="https://www.youtube.com/watch?v=JKZcYqZA8oo">[Youtube Link]</a> <a href="https://techcrunch.com/2019/11/05/adobes-project-sweet-talk-makes-portraits-come-alive/">[Press]</a></p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Aug. 2019] Our paper on <a href="#AnimationSkeleton">Animation Skeleton Prediction</a> accepted by 3DV 2019.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jul. 2019] Our paper <a href="#SceneGraphNet">SceneGraphNet</a> accepted by ICCV 2019.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jun. 2019] Joined Adobe CIL (Seattle) as a summer intern.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Jun. 2018] Joined Wayfair Next Research as a summer intern and fall co-op intern.</p>
								</div>
								<div class="desc" style="margin-left: 20px">
									<p>&#9658; [Apr. 2018] Our paper <a href="#VisemeNet">VisemeNet</a> accepted by SIGGRAPH 2018. <a href="https://www.youtube.com/watch?v=kk2EnyMD3mo">[Video]</a></p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-experience" data-section="publication">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Academia</span>
							<h2 class="colorlib-heading">Publications</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
				         <div class="timeline-centered">
				         	<article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">
					               <div class="timeline-label">
										<h2><a>MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</a> <span>2021-2022</span></h2>
										<h5>Zhan X., Yang Zhou, Li Y., E. Kalogerakis</h5>
										<h5><i>SIGGRAPH ASIA 2022</i></h5>
										<div class="gallery"><a><img src="./images/morig.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
										<p>We present MoRig, a method that automatically rigs character meshes driven by single-view point cloud streams capturing the motion of performing characters. Our method is also able to animate the 3D meshes according to the captured point cloud motion. At the heart of our approach lies a deep neural network that encodes motion cues from the point clouds into features that are informative about the articulated parts of the performing character. These features guide the inference of an appropriate skeletal rig for the input mesh, which is then animated based on the input point cloud motion. Our method can rig and animate diverse characters, including humanoids, quadrupeds, and toys with varying articulations. It is designed to account for occluded regions in the input point cloud sequences and any mismatches in the part proportions between the input mesh and captured character.</p>
										<p><a href="https://zhan-xu.github.io/motion-rig/">[Project Page]</a>
										<a href="https://arxiv.org/abs/2210.09463">[Paper]</a>
										<a href="https://github.com/zhan-xu/MoRig">[Code]</a></p>
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <div class="timeline-label">
					                  <h2><a>Skeleton-free Pose Transfer for Stylized 3D Characters</a> <span>2021-2022</span></h2>
					                  <h5>Z. Liao, J. Yang, J. Saito, G. Pons-Moll, Yang Zhou</h5>
					                  <h5><i>ECCV 2022</i></h5>
					                  <div class="gallery"><a><img src="./images/skel-free_motion_trans.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We present the first method that automatically transfers poses between stylized 3D characters without skeletal rigging. In contrast to previous attempts to learn pose transformations on fixed or topology-equivalent skeleton templates, our method focuses on a novel scenario to handle skeleton-free characters with diverse shapes, topologies, and mesh connectivities. The key idea of our method is to represent the characters in a unified articulation model so that the pose can be transferred through the correspondent parts. To achieve this, we propose a novel pose transfer network that predicts the character skinning weights and deformation transformations jointly to articulate the target character to match the desired pose. Our method is trained in a semi-supervised manner absorbing all existing character data with paired/unpaired poses and stylized shapes. It generalizes well to unseen stylized characters and inanimate objects.</p>
					                  <p><a href="https://zycliao.com/sfpt/">[Project Page]</a>
					                  	<a href="https://zycliao.com/sfpt/sfpt.pdf">[Paper]</a>
					                  	<a href="https://github.com/zycliao/skeleton-free-pose-transfer">[Code]</a></p>
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <div class="timeline-label">
					                  <h2><a>Learning Visibility for Robust Dense Human Body Estimation</a> <span>2021-2022</span></h2>
					                  <h5>C. Yao, J. Yang, D. Ceylan, Y. Zhou, Yang Zhou, M. Yang</h5>
					                  <h5><i>ECCV 2022</i></h5>
					                  <div class="gallery"><a><img src="./images/visdb.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>Estimating 3D human pose and shape from 2D images is a crucial yet challenging task. In this work, we learn dense human body estimation that is robust to partial observations. We explicitly model the visibility of human joints and vertices in the x, y, and z axes separately. The visibility in x and y axes help distinguishing out-of-frame cases, and the visibility in depth axis corresponds to occlusions (either self-occlusions or occlusions by other objects). We obtain pseudo ground-truths of visibility labels from dense UV correspondences and train a neural network to predict visibility along with 3D coordinates. We show that visibility can serve as 1) an additional signal to resolve depth ordering ambiguities of self-occluded vertices and 2) a regularization term when fitting a human body model to the predictions.</p>
					                  <p>
					                  	<a href="https://arxiv.org/pdf/2208.10652.pdf">[Paper]</a>
					                  	<a href="https://github.com/chhankyao/visdb">[Code]</a></p>
					               </div>
					            </div>
					         </article>
				         	<article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <div class="timeline-label" id="MakeItTalk">
					                  <h2><a>Audio-driven Neural Gesture Reenactment with Video Motion Graphs</a> <span>2020-2022</span></h2>
					                  <h5>Yang Zhou, J. Yang, D. Li, J. Saito, D. Aneja, E. Kalogerakis</h5>
					                  <h5><i>CVPR 2022</i></h5>
					                  <div class="gallery"><a><img src="./images/video_reenact.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>Human speech is often accompanied by body gestures including arm and hand gestures. We present a method that reenacts a high-quality video with gestures matching a target speech audio. The key idea of our method is to split and re-assemble clips from a reference video through a novel video motion graph encoding valid transitions between clips. To seamlessly connect different clips in the reenactment, we propose a pose-aware video blending network which synthesizes video frames around the stitched frames between two clips. Moreover, we developed an audio-based gesture searching algorithm to find the optimal order of the reenacted frames. Our system generates reenactments that are consistent with both the audio rhythms and the speech content.</p>
					                  <p><a href="https://yzhou359.github.io/video_reenact/">[Project Page]</a>
					                  	<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Audio-Driven_Neural_Gesture_Reenactment_With_Video_Motion_Graphs_CVPR_2022_paper.pdf">[Paper]</a>
					                  	<a href="https://github.com/yzhou359/vid-reenact">[Code]</a></p>
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <div class="timeline-label" id="MakeItTalk">
					                  <h2><a>APES: Articulated Part Extraction from Sprite Sheets</a> <span>2021-2022</span></h2>
					                  <h5>Z. Xu, M. Fisher, Yang Zhou, D. Aneja, R. Dudhat, L. Yi, E. Kalogerakis</h5>
					                  <h5><i>CVPR 2022</i></h5>
					                  <div class="gallery"><a><img src="./images/APES.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>Rigged puppets are one of the most prevalent representations to create 2D character animations. Creating these puppets requires partitioning characters into independently moving parts. In this work, we present a method to automatically identify such articulated parts from a small set of character poses shown in a sprite sheet, which is an illustration of the character that artists often draw before puppet creation. Our method is trained to infer articulated body parts, e.g. head, torso and limbs, that can be re-assembled to best reconstruct the given poses. Our results demonstrate significantly better performance than alternatives qualitatively and quantitatively.</p>
					                  <p><a href="https://zhan-xu.github.io/parts/">[Project Page]</a>
					                  	<a href="https://arxiv.org/abs/2206.02015">[Paper]</a>
					                  	<!-- <a href="https://github.com/yzhou359/vid-reenact">[Code]</a></p> -->
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <div class="timeline-label" id="MakeItTalk">
					                  <h2><a>MakeItTalk: Speaker-Aware Talking-Head Animation</a> <span>2019-2020</span></h2>
					                  <h5>Yang Zhou, X. Han, E. Shechtman, J. Echevarria, E. Kalogerakis, D. Li</h5>
					                  <h5><i>ACM SIGGRAPH ASIA, 2020</i></h5>
					                  <div class="gallery"><a><img src="./images/makeittalk_new.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We present a method that generates expressive talking heads from a single facial image with audio as the only input. Our method first disentangles the content and speaker information in the input audio signal. The audio content robustly controls the motion of lips and nearby facial regions, while the speaker information determines the specifics of facial expressions and the rest of the talking head dynamics. Our method is able to synthesize photorealistic videos of entire talking heads with full range of motion and also animate artistic paintings, sketches, 2D cartoon characters,  Japanese mangas, stylized caricatures in a single unified framework.</p>
					                  <p><a href="https://people.umass.edu/~yangzhou/MakeItTalk/">[Project Page]</a>
					                  	<a href="https://arxiv.org/abs/2004.12992">[Paper]</a>
					                  	<a href="https://youtu.be/OU6Ctzhpc6s">[Video]</a>
					                  	<a href="https://www.youtube.com/watch?v=vUMGKASgbf8">[New Video!]</a>
					                  	<a href="https://github.com/yzhou359/MakeItTalk">[Code]</a></p>
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <!--div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div -->

					               <div class="timeline-label" id="RigNet">
					                  <h2><a>RigNet: Neural Rigging for Articulated Characters</a> <span>2018-2019</span></h2>
					                  <h5>Z. Xu, Yang Zhou, E. Kalogerakis, C. Landreth, K. Singh</h5>
					                  <h5><i>ACM SIGGRAPH, 2020</i></h5>
					                  <div class="gallery"><a><img src="./images/rignet.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We present RigNet, an end-to-end automated method for producing animation rigs from input character models. Given an input 3D model representing an articulated character, RigNet predicts a skeleton that matches the animator expectations in joint placement and topology. It also estimates surface skin weights based on the predicted skeleton. Our method is based on a deep architecture that directly operates on the mesh representation without making assumptions on shape class and structure.</p>
					                  <p><a href="https://zhan-xu.github.io/rig-net/">[Project Page]</a>
					                  	<a href="https://www.youtube.com/watch?v=J90VETgWIDg&feature=emb_logo">[Video]</a>
					                  	<a href="https://github.com/zhan-xu/RigNet">[Code]</a>
					                  	<a href="images/rignet.pdf">[Paper]</a>
					                  	<!--
					                  <a href="https://github.com/yzhou359/3DIndoor-SceneGraphNet">[Code]</a> --></p>
					               </div>
					            </div>
					         </article>



				         	<article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <!--div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div -->

					               <div class="timeline-label" id="SceneGraphNet">
					                  <h2><a>SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation</a> <span>2018-2019</span></h2>
					                  <h5>Yang Zhou, Z. While, E. Kalogerakis</h5>
					                  <h5><i>International Conference Computer Vision (ICCV), 2019</i></h5>
					                  <div class="gallery"><a><img src="./images/scenegraphnet.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We propose a neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings. Given an input, potentially incomplete, 3D scene and a query location, our method predicts a probability distribution over object types that fit well in that location. Our distribution is predicted though passing learned messages in a dense graph whose nodes represent objects in the input scene and edges represent spatial and structural relationships.</p>
					                  <p><a href="https://people.umass.edu/~yangzhou/scenegraphnet/">[Project Page]</a>
					                  	<a href="https://arxiv.org/abs/1907.11308">[Paper]</a>
					                  <a href="https://github.com/yzhou359/3DIndoor-SceneGraphNet">[Code]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <!--div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div -->

					               <div class="timeline-label" id="AnimationSkeleton">
					                  <h2><a>Predicting Animation Skeletons for 3D Articulated Models via Volumetric Nets</a> <span>2018-2019</span></h2>
					                  <h5>Z. Xu, Yang Zhou, E. Kalogerakis, K. Singh</h5>
					                  <h5><i>International Conference on 3D Vision (3DV) 2019</i></h5>
					                  <div class="gallery"><a><img src="./images/3dv_rigging.png" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We present a learning method for predicting animation skeletons for input 3D models of articulated characters. In contrast to previous approaches that fit pre-defined skeleton templates or predict fixed sets of joints, our method produces an animation skeleton tailored for the structure and geometry of the input 3D model.</p>
					                  <p><a href="https://people.cs.umass.edu/~zhanxu/projects/AnimSkelVolNet//">[Project Page]</a>
					                  <a href="https://github.com/zhan-xu/AnimSkelVolNet">[Code]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">

					               <!--div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div -->

					               <div class="timeline-label" id="VisemeNet">
					                  <h2><a>VisemeNet: Audio-Driven Animator-Centric Speech Animation</a> <span>2016-2018</span></h2>
					                  <h5>Yang Zhou, Z. Xu, C. Landreth, S. Maji, E. Kalogerakis, K. Singh</h5>
					                  <h5><i>ACM SIGGRAPH, 2018 (also appears on ACM Trans. on Graphics, 37(4), 2018)</i></h5>
					                  <div class="gallery"><a><img src="./images/visemenet.jpg" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio.</p>
					                  <p><a href="https://people.umass.edu/~yangzhou/visemenet/">[Project Page]</a>
					                  	<a href="https://arxiv.org/abs/1805.09488">[Paper]</a>
					                  <a href="https://github.com/yzhou359/VisemeNet_tensorflow">[Code]</a>
					              <a href="https://www.youtube.com/watch?v=kk2EnyMD3mo">[Video]</a></p>
					               </div>
					            </div>
					         </article>


					         <article class="timeline-entry" data-animate-effect="fadeInRight">
					            <div class="timeline-entry-inner">
					               <!-- div class="timeline-icon color-2">
					                  <i class="icon-pen2"></i>
					               </div -->
					               <div class="timeline-label">
					               	<h2><a>Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55</a> <span>2017</span></h2>
					                  <h5>L. Yi, L. Shao, M. Savva, H. Huang, Yang Zhou, et al.</h5>
					                  <h5><i>International Conference Computer Vision Workshop (ICCVW), 2017</i></h5>
					                  <div class="gallery"><a><img src="./images/shapenet.jpg" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes. We collaborate with ShapeNet team in helping building the training and testing dataset of “Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55”. In particular, we help check the geometry duplicates in ShapeNet Core dataset.</p>
					                  <p><a href="https://shapenet.cs.stanford.edu/iccv17/">[3D Shape Reconstruction and Segmentation Task Page]</a>
					                  	<a href="https://arxiv.org/abs/1710.06104">[Paper]</a>
					                  <a href="http://antares.cs.umass.edu/project_data/AdversarialMonsters/Duplicate/ShapeNetDuplicate/index-bsr-exterior.php?category=02691156">[ShapeNet Duplicate Check]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">
					               <!-- div class="timeline-icon color-3">
					                  <i class="icon-pen2"></i>
					               </div -->
					               <div class="timeline-label">
					               	<h2><a>A Tube-and-Droplet-based Approach for Representing and Analyzing Motion Trajectories</a> <span>2014-2016</span></h2>
					                  <h5>W. Lin, Yang Zhou, H. Xu, J. Yan, M. Xu, J. Wu, Z. Liu</h5>
					                  <h5><i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), 39(8), pp. 1489-1503, 2017</i></h5>
					                  <div class="gallery"><a><img src="./images/tubelet.jpg" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We address the problem of representing motion trajectories in a highly informative way, and consequently utilize it for analyzing trajectories. We apply our tube-and-droplet representation to trajectory analysis applications including trajectory clustering, trajectory classification & abnormality detection, and 3D action recognition.</p>
					                  <p><a href="http://min.sjtu.edu.cn/lwydemo/Trajectory%20analysis.htm">[Project Page]</a>
					                  	<a href="https://arxiv.org/abs/1609.03058">[Paper]</a>
					                  	<a href="http://min.sjtu.edu.cn/lwydemo/Trajectory%20analysis/TRAFFIC_dataset.zip">[Dataset]</a>
					                  	<a href="http://min.sjtu.edu.cn/lwydemo/Trajectory%20analysis/3D-tube%20code.zip">[Code]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry animate-b" data-animate-effect="fadeInTop">
					            <div class="timeline-entry-inner">
					               <!-- div class="timeline-icon color-4">
					                  <i class="icon-pen2"></i>
					               </div -->
					               <div class="timeline-label">
					               	<h2><a >Unsupervised Trajectory Clustering via Adaptive Multi-Kernel-based Shrinkage</a> <span>2014-2015</span></h2>
					               	<h5>H. Xu, Yang Zhou, W. Lin, H. Zha</h5>
					                  <h5><i>International Conference Computer Vision (ICCV), pp. 4328-4336, 2015</i></h5>
					                  <div class="gallery"><a><img src="./images/shrink.jpg" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p>We introduce an adaptive multi-kernel-based estimation process to estimate the 'shrunk' positions and speeds of trajectories' points. This kernel-based estimation effectively leverages both multiple structural information within a trajectory and the local motion patterns across multiple trajectories, such that the discrimination of the shrunk point can be properly increased.</p>
					                  <p><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Xu_Unsupervised_Trajectory_Clustering_ICCV_2015_paper.html">[Paper]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry" data-animate-effect="fadeInLeft">
					            <div class="timeline-entry-inner">
					               <!-- div class="timeline-icon color-5">
					                  <i class="icon-pen2"></i>
					               </div -->
					               <div class="timeline-label">
					               	<h2><a>Representing and recognizing motion trajectories: a tube and droplet approach</a> <span>2013-2014</span></h2>
					                  <h5>Yang Zhou, W. Lin, H. Su, J. Wu, J. Wang, Y. Zhou</h5>
					                  <h5><i>ACM Intl. Conf. on Multimedia (MM), pp. 1077-1080. 2014</i></h5>
					                  <div class="gallery"><a><img src="./images/acmmm.jpg" style="max-height: 300px; max-width: 95%; margin: 10px 0 10px 0;"></a></div>
					                  <p> This paper addresses the problem of representing and recognizing motion trajectories. We propose a 3D tube which can effectively embed both motion and scene-related information of a motion trajectory and a droplet-based method which can suitably catch the characteristics of the 3D tube for activity recognition.</p>
					                  <p><a href="https://cs.nju.edu.cn/wujx/paper/ACM_MM_2014.pdf">[Paper]</a></p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry begin" data-animate-effect="fadeInBottom">
					            <!--div class="timeline-entry-inner">
					               <div class="timeline-icon color-none">
					               </div>
					            </div -->
					         </article>
					      </div>
					   </div>
				   </div>
				</div>
			</section>


			<section class="colorlib-blog" data-section="work" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Work</span>
							<h2 class="colorlib-heading">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Media Intelligent Lab</a></h3>
									<span><small>May 2021 </small> | <small> Research Scientist </small> </span>
									<p>Working on digital human related projects.</p>
									<!-- <p><b>Body Turner Behavior</b> shipped in Adobe Character Animator. <a href="https://helpx.adobe.com/adobe-character-animator/using/whats-new.html">[Link]</a>  -->
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Media Intelligent Lab</a></h3>
									<span><small>June, 2020 </small> | <small> Research Intern </small> </span>
									<p>Collaborate with researchers on 3D facial/skeleton animations based on deep learning approaches.</p>
									<p>Our intern project #OnTheBeatSneak was presented at <a href="https://blog.adobe.com/en/publish/2020/10/21/max-sneaks-2020-where-creativity-and-innovation-knows-no-bounds.html#gs.kjk07b">Adobe MAX 2020 (Sneak Peek)</a>.</p>
									<p><a href="https://www.youtube.com/watch?v=R-0w3IuGEKU&feature=emb_title">[Quick Look]</a> <a href="https://www.youtube.com/watch?v=NMHLAVjyFxo">[Full Youtube Link]</a> <a href="https://www.protocol.com/adobe-max-ai-video-editing">[Press]</a></p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/adobe.png" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://www.adobe.com/">Adobe, Inc | Creative Intelligence Lab</a></h3>
									<span><small>June, 2019 </small> | <small> Research Intern </small> </span>
									<p>Collaborate with researchers on audio-driven cartoon and real human facial animations and lip-sync technologies based on deep learning approaches.</p>
									<p>Our intern project #SweetTalk was presented at <a href="https://theblog.adobe.com/adobe-max-sneaks-2019/?scid=4e31cfc1-b79d-4cb4-b0e3-cc4907d0e054&mv=social&mv2=owned_social">Adobe MAX 2019 (Sneak Peek)</a>.</p>
									<p><a href="https://www.youtube.com/watch?v=JKZcYqZA8oo">[Youtube Link]</a> <a href="https://techcrunch.com/2019/11/05/adobes-project-sweet-talk-makes-portraits-come-alive/">[Press]</a></p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/wayfair.jpg" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://tech.wayfair.com/category/wayfair-next/">Wayfair, Inc | Wayfair Next Research</a></h3>
									<span><small>June, 2018 </small> | <small> Research Intern </small> </span>
									<p>Working on 3D scene systhesis based on deep learning approaches.</p>
								</div>
							</div>
						</div>
						<div class="" data-animate-effect="fadeInLeft">
						<div class="blog-entry" style="width: 15%; text-align:center">
							<a href="blog.html" class="blog-img"><img src="images/netease.jpg" style="max-width: 80%;" center></a>
						</div>
						<div class="blog-entry" style="width: 85%;">

								<div class="desc">
									<h3><a href="https://game.163.com/en/">NetEase Game, Inc</a></h3>
									<span><small>June, 2015 </small> | <small> Management Trainee </small> </span>
									<p>Working on mobile game design, especially on profit models and user-experiences.</p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<!-- <section class="colorlib-work" data-section="work">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
							<span class="heading-meta">Work</span>
							<h2 class="colorlib-heading animate-box">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-6 animate-box" data-animate-effect="fadeInLeft">
							<div class="project" style="background-image: url(images/img-1.jpg);">
								<div class="desc">
									<div class="con">
										<h3><a href="work.html">Work 01</a></h3>
										<span>Website</span>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section> -->


			<section class="colorlib-contact" data-section="contact" style="padding-bottom: 20em;">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<span class="heading-meta">Best way to</span>
							<h2 class="colorlib-heading">Contact Me</h2>
						</div>
					</div>
					<div class="row">
						<div class="" data-animate-effect="fadeInLeft">
							<div class="blog-entry" style="width: 80%;">
								<div class="desc" style="margin-left: 20px">
									<p>Best way to reach me is to send an <a href="mailto:yazhou@adobe.com/">Email</a></p>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>


	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

